{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQYNY8vwr93x",
        "outputId": "fcb1b130-b25b-438f-99fd-5ab1fd340646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer,BertForQuestionAnswering\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "teHyaKPMsPUz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, db"
      ],
      "metadata": {
        "id": "T1cG7Rs1tpBZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "firebase_initialization = False"
      ],
      "metadata": {
        "id": "XgMJtRngswh0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize(): #run only once-per iteration\n",
        "  if(firebase_initialization == False):\n",
        "    cred_obj = credentials.Certificate(\"/content/naanbert-firebase-adminsdk-k1t8m-65d91682f0.json\")\n",
        "    default_app = firebase_admin.initialize_app(cred_obj,{'databaseURL': 'https://naanbert-default-rtdb.firebaseio.com/'})"
      ],
      "metadata": {
        "id": "l3ZvlU0rtICP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initialize()"
      ],
      "metadata": {
        "id": "URV_KMcotspl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(key):\n",
        "  ref = db.reference('/')\n",
        "  data = ref.get()\n",
        "  abstract = data[key]\n",
        "  return abstract\n",
        "print(get_context('Introduction'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU2k-WNLtbIb",
        "outputId": "8247513b-ac0b-4186-c755-7336da9d888b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aravinda Eyecare System's vision is to eliminate needless blindness. The mission of Aravinda Eyecare System is to eliminate needless blindness by providing compassionate and quality eye care affordable to all. NABH stands for National Accreditation Board for Hospitals and Healthcare providers. It is a constituent board of the Quality Council of India (QCI) and was set up to establish and operate an accreditation programme for healthcare organizations in India. Initially, it was a voluntary programme but now includes mandated entry-level standards. NABH accredits all types of healthcare delivery organizations, including blood banks, imaging centers, AYUSH, nursing homes, clinics, and more. The NABH accreditation process involves three levels: Pre-accreditation entry level, Pre-accreditation progressive level, and Full accreditation. Healthcare organizations can be classified into two main categories: Small healthcare organizations (SHCO), which have less than 50 beds, and larger healthcare organizations (HCO), which have more than 50 beds. Small healthcare organizations (SHCO) under Aravinda Eyecare System are located in Tirupur, Dindigul, Tuticorin, and Udumalpet. Larger healthcare organizations (HCO) are located in Madurai, Tirunelveli, Coimbatore, Pondicherry, Theni, and Salem. The constituents of accreditation include Structure, Process, and Outcome. NABH focuses on ensuring safety in several areas including patient safety, employee safety, visitor safety, and physical safety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oobyIU8sUjQ",
        "outputId": "7a5dbd68-17c6-463b-8440-9a0dd99315b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
      ],
      "metadata": {
        "id": "xcS6n4PGsZW3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(question, answer_text)"
      ],
      "metadata": {
        "id": "ZUpYZDytsk-U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example through the model.\n",
        "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                    return_dict=True)\n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "\n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "\n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "-4RFTfNGsoVG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract = get_context(\"Introduction\")\n",
        "wrapper = textwrap.TextWrapper(width=80)"
      ],
      "metadata": {
        "id": "sSiEu8c0s2B4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"organizations that NABH accredit\"\n",
        "\n",
        "answer_question(question,abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8giuU1Gs41z",
        "outputId": "ed5a2414-5c6e-44bf-f2e2-12df10ad3545"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 313 tokens.\n",
            "\n",
            "Answer: \"nabh accredits all types of healthcare delivery organizations\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jb4SZ_f5s6Xl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}